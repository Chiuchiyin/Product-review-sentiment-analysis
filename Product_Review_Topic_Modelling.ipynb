{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaySZigQAc_l"
   },
   "source": [
    "# Product Review Topic Modelling\n",
    "\n",
    "A report focused on Topic Modeling using [tmtoolkit](https://tmtoolkit.readthedocs.io/) to process text review data from Amazon for clothing and shoes manufactured by Nike. Use unsupervised LDA clustering methods to create popular topics in the Amazon review data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEuxfGTAAEWQ"
   },
   "source": [
    "**Context**\n",
    "Online marketplaces and applications can product data at volumes too big for a human to read and analyze. An example of this is Amazon, where millions of customers buy and review products. When a company wants to extract insights from their reviews, they need a way to process the text of the reviews and identifty patterns. This exploratory analysis of big data is a common use case for Topic Modeling using unsupervised learning.\n",
    "\n",
    "**Background**\n",
    "The Nike brand is interested in extracting key indsights from customer review data on the Amazon website. Specifically, they would like to understand when customers are not satisfied and what major problems they face with their products. To accomplish this task, the Data Science team at Nike is tasked with analyzing the Amazon review data. First, the team must identify Nike's product ASINs on Amazon and extract the relevant reviews. Next, the team will complete topic modeling on the text of the reviews using the LDA clustering method to model the data into popular topics. Finally, the team will vizualize the topics and derive initial insights to drive the strategy of the team.\n",
    "\n",
    "** LDA (Latent Dirichleyt Allocation)**\n",
    "This project will focus on the application of [lda.LDA](https://github.com/lda-project/lda), which implements latent Dirichlet allocation (LDA).\n",
    "\n",
    "Specifically, in natural language processing (NLP), Latent Dirichlet Allocation (LDA) is a Bayesian network used for topic modeling. It aims to explain a set of observations through unobserved groups. Each group explains why some parts of the data are similar. For most purposes, observations are words and are collected into documents. Each word's presence is attributed to one of the document's topics. Each document will contain a small number of topics. [1]\n",
    "\n",
    "**Data Source**\n",
    "Prof. Julian McAuley at UC-San Diego created an [Amazon Product Data](http://jmcauley.ucsd.edu/data/amazon/links.html) database, which contains Amazon products details. This project will leverage 2 datasets from the database:\n",
    "- (1) meta-data about products - details related to each Amazon product\n",
    "- (2) product reviews - reviews on all types of Amazon products\n",
    "\n",
    "Since the overall size of these datasets are huge (~80gb), a subset of data will be utilized to focus on the product category of **Clothing, Shoes & Jewelry**.\n",
    "\n",
    "The raw data sources for the project can be accessed with the following links:\n",
    "- [Product Data](http://128.138.93.164/meta_Clothing_Shoes_and_Jewelry.json.gz)\n",
    "- [Review Data](http://128.138.93.164/reviews_Clothing_Shoes_and_Jewelry.json.gz)\n",
    "\n",
    "**Overview of Observations**\n",
    "\n",
    "The **Product Details** dataset has 1,503,384 records in a json format and includes the following fields:\n",
    "- asin: a unique product id assigned to the Amazon product\n",
    "- title: the name of the product displayed on the amazon page\n",
    "- imUrl: a URL to access the product page\n",
    "- related: related products to the this product\n",
    "- salesRank: the main sales category and the product rank within the sales category\n",
    "- categories: categories or tags grouping the products\n",
    "\n",
    "The **Review Details** dataset has 5,748,920 records in a json format and includes the following fields:\n",
    "- reviewerID': a unique id assigned to the review\n",
    "- asin': a unique id of the product reviewed\n",
    "- reviewerName': the text name of the user who created the review\n",
    "- helpful': indicator if the review was helpful or not helpful\n",
    "- reviewText': the text written for the review\n",
    "- overall': the rating of the product (in stars) on a scale of 1-5\n",
    "- summary': the summary or title of the review\n",
    "- unixReviewTime: the numeric unix representation of the time the review was created\n",
    "- reviewTime: the time the review was created as a formatted data string\n",
    "\n",
    "**Objective**\n",
    "The objective is to build a unsupervize topic model for nike product reviews. The topic model will group reviews into topics and create label to identify patterns and trends in user reviews. Final evaluation will use qualitative and look at value of the topics modeled and the usefullness of insights extracted from the topics.\n",
    "\n",
    "**Report Overview**\n",
    "The project will cover 5 key phases:\n",
    "1. Data Source: Extracting, filtering, and focusing the data on the Nike brand\n",
    "2. Preprocessing: Extracting Word Features with Natural Language Processing (NLP) tools\n",
    "3. Parameter Tuning: Tuning the topic model parameters to improve the purity and uniqueness of topics\n",
    "4. Final Model: Building the final topic model\n",
    "5. Classify and Enright Topic Data: tuning the topic labels, tagging all records, and adding in supporting data to the final dataset\n",
    "6. Model Evaluation: Review the model and extract insights for the Nike Brand\n",
    "7. Results: Review the findings of the Topic Evaluation.\n",
    "\n",
    "## Setting Up\n",
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4DfKV1oAY5o",
    "outputId": "4ffe07ce-a6f3-4a53-ca61-cd97f7e184f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] Importing packages...\n"
     ]
    }
   ],
   "source": [
    "# Special Install of Packages\n",
    "print('[-] Importing packages...')\n",
    "#special_install_tmtoolkit\n",
    "import os\n",
    "try:\n",
    "    import tmtoolkit\n",
    "except:\n",
    "  print('starting patch of tmtoolkit.')\n",
    "  !pip install --quiet -U \"tmtoolkit[recommended,lda,sklearn,wordclouds,topic_modeling_eval_extra]\"\n",
    "  print('finished patch of tmtoolkit.')\n",
    "  os.kill(os.getpid(), 9)\n",
    "\n",
    "#special_install_lda\n",
    "import os\n",
    "try:\n",
    "  from tmtoolkit.topicmod.tm_lda import compute_models_parallel\n",
    "except:\n",
    "  !pip install --quiet tmtoolkit['lda']\n",
    "  from tmtoolkit.topicmod.tm_lda import compute_models_parallel\n",
    "\n",
    "try:\n",
    "  from lda import LDA\n",
    "except:\n",
    "  !pip install --quiet lda\n",
    "  from lda import LDA\n",
    "\n",
    "#special_install_pyLDAvis\n",
    "try:\n",
    "  import pyLDAvis\n",
    "except:\n",
    "  !pip install --quiet pyLDAvis==2.1.2\n",
    "  import pyLDAvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "VoJRWDFiAqqe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] Importing packages...\n"
     ]
    }
   ],
   "source": [
    "print('[-] Importing packages...')\n",
    "# File Connection and File Manipulation\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "# Import Usability Functions\n",
    "import logging\n",
    "import warnings\n",
    "# Basic Data Science Toolkits\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from time import sleep\n",
    "# Basic Data Vizualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Text Preprocessing (tmtoolkit)\n",
    "import tmtoolkit\n",
    "from tmtoolkit.corpus import Corpus, lemmatize, to_lowercase, remove_chars, filter_clean_tokens\n",
    "from tmtoolkit.corpus import filter_for_pos, remove_common_tokens, remove_uncommon_tokens\n",
    "from tmtoolkit.corpus import corpus_num_tokens, corpus_tokens_flattened\n",
    "from tmtoolkit.corpus import doc_tokens, tokens_table, doc_labels, dtm\n",
    "from tmtoolkit.corpus import vocabulary, vocabulary_size, vocabulary_counts\n",
    "from tmtoolkit.topicmod.model_io import print_ldamodel_topic_words\n",
    "from tmtoolkit.topicmod.tm_lda import compute_models_parallel\n",
    "from tmtoolkit.corpus.visualize import plot_doc_lengths_hist, plot_doc_frequencies_hist, plot_ranked_vocab_counts\n",
    "#https://tmtoolkit.readthedocs.io/en/latest/preprocessing.html\n",
    "# Text Preprocessing(other)\n",
    "from string import punctuation\n",
    "import nltk\n",
    "import scipy.sparse\n",
    "# Topic Modeling\n",
    "from lda import LDA\n",
    "import pyLDAvis\n",
    "from tmtoolkit.topicmod import tm_lda\n",
    "from tmtoolkit.topicmod.tm_lda import compute_models_parallel\n",
    "from tmtoolkit.topicmod.model_io import print_ldamodel_topic_words\n",
    "from tmtoolkit.topicmod.model_io import save_ldamodel_to_pickle\n",
    "from tmtoolkit.topicmod.model_io import load_ldamodel_from_pickle\n",
    "from tmtoolkit.topicmod.model_io import ldamodel_top_doc_topics\n",
    "from tmtoolkit.topicmod.evaluate import results_by_parameter\n",
    "from tmtoolkit.topicmod.visualize import plot_eval_results\n",
    "from tmtoolkit.topicmod.visualize import parameters_for_ldavis\n",
    "from tmtoolkit.topicmod.visualize import generate_wordclouds_for_topic_words\n",
    "from tmtoolkit.topicmod.model_stats import generate_topic_labels_from_top_words\n",
    "from tmtoolkit.bow.bow_stats import doc_lengths\n",
    "# Sentiment Modeling\n",
    "from textblob import TextBlob\n",
    "# normalize\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGSpO4krARX9"
   },
   "source": [
    "### Set Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "dWA_xDHjAyA_"
   },
   "outputs": [],
   "source": [
    "random.seed(20191120)   # to make the sampling reproducible\n",
    "np.set_printoptions(precision=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIK88FsNAeT7"
   },
   "source": [
    "### Verify GPU Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "FJ6fPCw7AZcz",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 25 18:17:59 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.52                 Driver Version: 551.52         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   47C    P0             32W /  115W |     529MiB /   8192MiB |      5%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2024    C+G   C:\\Windows\\System32\\dwm.exe                 N/A      |\n",
      "|    0   N/A  N/A      8152    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     16440    C+G   ...x86__97hta09mmv6hy\\Build\\Lively.exe      N/A      |\n",
      "|    0   N/A  N/A     16764    C+G   ...Brave-Browser\\Application\\brave.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "HONfWtNGCAlQ"
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = \"./\"\n",
    "DATA_DIR = \"%s/data\" % ROOT_DIR\n",
    "EVAL_DIR = \"%s/evaluation\" % ROOT_DIR\n",
    "MODEL_DIR = \"%s/models\" % ROOT_DIR\n",
    "\n",
    "#Create missing directories, if they don't exist\n",
    "if not os.path.exists(DATA_DIR):\n",
    "  # Create a new directory because it does not exist\n",
    "  os.makedirs(DATA_DIR)\n",
    "  print(\"The data directory is created!\")\n",
    "if not os.path.exists(EVAL_DIR):\n",
    "  # Create a new directory because it does not exist\n",
    "  os.makedirs(EVAL_DIR)\n",
    "  print(\"The evaluation directory is created!\")\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "  # Create a new directory because it does not exist\n",
    "  os.makedirs(MODEL_DIR)\n",
    "  print(\"The model directory is created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mG2bHve1_7Ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/meta_Clothing_Shoes_and_Jewelry.jsonl.gz',\n",
       " <http.client.HTTPMessage at 0x1e28ab25790>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#link would be expired by the time of the grading. It's just here to show how it could be done.\n",
    "import urllib.request\n",
    "url = 'https://d3c33hcgiwev3.cloudfront.net/F4BxPB4wTWSAcTweMH1ktA_cf7170556c97498f80af8f7b869f35f1_meta_Clothing_Shoes_and_Jewelry.jsonl.gz?Expires=1711497600&Signature=ABbwkDeNeTL-s2uHXlyu4QweI8JnGGaCopj50~hba0MaEMhk70IImmREl5hzHC6O0mxPkFACpq5PP~JX8XnIEVe1cph7G2neQ0zgW1Q9npt47H-RMjk0p6emo95c8oZ~aWty9kEQBZM3D7PqnwRe7dHRWbZ-eS84gYkvdAq0w48_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A'\n",
    "filename = './data/meta_Clothing_Shoes_and_Jewelry.jsonl.gz'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/reviews_Clothing_Shoes_and_Jewelry.json.gz',\n",
       " <http.client.HTTPMessage at 0x1e28b6f9ed0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://d3c33hcgiwev3.cloudfront.net/ed_p_0DhRh-f6f9A4dYfJA_d374df8f88084b8c9384c9b910b50cf1_reviews_Clothing_Shoes_and_Jewelry.json.gz?Expires=1711497600&Signature=Nllx8wiA27Ey2uOdCpLCRGy-y31pBWRnlXG5AtnpXxVoxPLABKJBTsk4eAcUIGTPzw23f6zckeNTtAbIfOy9w8mCGa4kjFDZAKFAaLXzcmUEs4ia09dzaSYjuduIHhE4ECVvr~VyutJkqfS4MwXxWpgqvwbB44F9RqE5g3P3h6A_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A'\n",
    "filename = './data/reviews_Clothing_Shoes_and_Jewelry.json.gz'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fiiUVkmJAOch"
   },
   "outputs": [],
   "source": [
    "meta_file_path = '%s/meta_Clothing_Shoes_and_Jewelry.jsonl.gz' % DATA_DIR\n",
    "review_file_path = '%s/reviews_Clothing_Shoes_and_Jewelry.json.gz' % DATA_DIR\n",
    "\n",
    "!gzip -d \"$meta_file_path\"\n",
    "!gzip -d \"$review_file_path\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Product Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##this assigns the filename we're trying to load in to a string variable\n",
    "meta_file_path = '%s/meta_Clothing_Shoes_and_Jewelry.jsonl' % DATA_DIR\n",
    "loadedjson = open(meta_file_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading product data to dictionary:\n",
      "[-] current progress: 100000 and a runtime of 24.194 seconds.\n",
      "[-] current progress: 200000 and a runtime of 36.596 seconds.\n",
      "[-] current progress: 300000 and a runtime of 48.865 seconds.\n",
      "[-] current progress: 400000 and a runtime of 61.053 seconds.\n",
      "[-] current progress: 500000 and a runtime of 72.612 seconds.\n",
      "[-] current progress: 600000 and a runtime of 85.12 seconds.\n",
      "[-] current progress: 700000 and a runtime of 98.269 seconds.\n",
      "[-] current progress: 800000 and a runtime of 111.138 seconds.\n",
      "[-] current progress: 900000 and a runtime of 188.012 seconds.\n",
      "[-] current progress: 1000000 and a runtime of 201.046 seconds.\n",
      "[-] current progress: 1100000 and a runtime of 216.22 seconds.\n",
      "[-] current progress: 1200000 and a runtime of 230.819 seconds.\n",
      "[-] current progress: 1300000 and a runtime of 246.186 seconds.\n",
      "[-] current progress: 1400000 and a runtime of 262.559 seconds.\n",
      "[-] current progress: 1500000 and a runtime of 343.536 seconds.\n"
     ]
    }
   ],
   "source": [
    "#The data used in this script comes from: http://jmcauley.ucsd.edu/data/amazon/links.html\n",
    "#The data here is the 'per category' data for Clothing, Shoes and Jewelry\n",
    "#use the above url to better understand the data, where it came from, and some\n",
    "#tips on how to use it!\n",
    "\n",
    "#getting reviews is going to be a two step process:\n",
    "#1) go through the amazon product catalog for \"Clothing, Shoes and Jewelery\n",
    "#and extract out matching products by their ASIN\n",
    "#2) go through the review data and parse out the matching reviews by ASIN\n",
    "\n",
    "#1) - Extracting ASINs by brand\n",
    "#First, let's iterate through the data and store it as a python dictionary\n",
    "\n",
    "#let's set a counter to see how many products we have in the json\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "#loading the json file\n",
    "#we've always got to initiate dictionaries before we can use them\n",
    "allproducts = {}\n",
    "\n",
    "#each line of data here is a product and its metadata\n",
    "print('loading product data to dictionary:')\n",
    "for aline in loadedjson:\n",
    "    #creating a counter to know our progress in processing the entire catalog\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        #we're only going to print our count every 100k, this way we don't spam\n",
    "        #our output console\n",
    "        current_runtime = round(time.time() - start_time,3)\n",
    "        print('[-] current progress:', count, 'and a runtime of', current_runtime, 'seconds.')\n",
    "    #interestingly enough, this data isn't true JSON, instead it's python\n",
    "    #dictionaries that have essentially been printed as text. It's odd, but if\n",
    "    #we read the documentaion, all we need to do to load a dictionary is use\n",
    "    #the eval function. https://www.programiz.com/python-programming/methods/built-in/eval\n",
    "    #eval takes whatever string is passed to it, and interprets it as python code\n",
    "    #and runs it. So here, it's exactly what we need to interpret a printed\n",
    "    #python dictionary\n",
    "\n",
    "    aproduct = eval(aline)\n",
    "\n",
    "    #making a dictionary entry with the ASIN of the product as the key\n",
    "    #and it's metadata as nested dictionaries\n",
    "    allproducts[aproduct['asin']] = aproduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed for 1503384 of 1503384 records with a final runtime of 344.088 seconds.\n"
     ]
    }
   ],
   "source": [
    "#print a summary of the records processed\n",
    "allproducts_length = len(allproducts)\n",
    "current_runtime = round(time.time() - start_time,3)\n",
    "print('Process completed for', count, 'of', allproducts_length, 'records with a final runtime of', current_runtime, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asin': 'B00KUSKHDC',\n",
       " 'title': \"Family Guy - Men's T-shirt Evil Monkey\",\n",
       " 'imUrl': 'http://ecx.images-amazon.com/images/I/41eUK6CAY4L._SX342_.jpg',\n",
       " 'related': {'also_viewed': ['B004P0JEK8',\n",
       "   'B00EC4UZ3M',\n",
       "   'B000VYZEY2',\n",
       "   'B00HZSI7QE']},\n",
       " 'salesRank': {'Clothing': 288020},\n",
       " 'categories': [['Clothing, Shoes & Jewelry', 'Men'],\n",
       "  ['Clothing, Shoes & Jewelry',\n",
       "   'Novelty, Costumes & More',\n",
       "   'Novelty',\n",
       "   'Clothing',\n",
       "   'Men',\n",
       "   'Shirts',\n",
       "   'T-Shirts']]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview the product record\n",
    "allproducts['B00KUSKHDC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the files to disk\n",
    "allproducts_file_path = '%s/allproducts.p' % DATA_DIR\n",
    "pickle.dump(allproducts, open(allproducts_file_path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the Product Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading categories data to dictionary:\n",
      "[-] current progress: 7 % and a runtime of 0.35 seconds.\n",
      "[-] current progress: 13 % and a runtime of 0.699 seconds.\n",
      "[-] current progress: 20 % and a runtime of 1.029 seconds.\n",
      "[-] current progress: 27 % and a runtime of 1.371 seconds.\n",
      "[-] current progress: 33 % and a runtime of 1.679 seconds.\n",
      "[-] current progress: 40 % and a runtime of 1.985 seconds.\n",
      "[-] current progress: 47 % and a runtime of 2.316 seconds.\n",
      "[-] current progress: 53 % and a runtime of 2.61 seconds.\n",
      "[-] current progress: 60 % and a runtime of 2.891 seconds.\n",
      "[-] current progress: 67 % and a runtime of 3.171 seconds.\n",
      "[-] current progress: 73 % and a runtime of 3.46 seconds.\n",
      "[-] current progress: 80 % and a runtime of 3.736 seconds.\n",
      "[-] current progress: 86 % and a runtime of 4.01 seconds.\n",
      "[-] current progress: 93 % and a runtime of 4.28 seconds.\n",
      "[-] current progress: 100 % and a runtime of 4.546 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Next we need to explore the product data to see what categories are common in the\n",
    "#data. As you'll learn, product categories are wishywashy in that they can be\n",
    "#product categories (e.g., baby, house and home), or they can be brands!\n",
    "#We're already dealing with a subset of the product categories, Clothing, Shoes\n",
    "#and Jewlery. We still need to find a list of product ids for our specific\n",
    "#brand. To do this,We're going to use the 'categories' metadata field to find\n",
    "#your brand\n",
    "\n",
    "##Let's create a dictionary of all the product subcategories\n",
    "#and by doing so, also come up with a list of brands and the number of products\n",
    "#they have listed in the amazon product catalog\n",
    "\n",
    "allcategories = {}\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "#each line of data here is a product and its metadata\n",
    "print('loading categories data to dictionary:')\n",
    "for aproduct in allproducts:\n",
    "    #creating a counter to know our progress in processing the entire catalog\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        #we now know there are 1.5 million products, so we can build a counter\n",
    "        #that tells how our processing is going. When the counter reaches one\n",
    "        #we're done!\n",
    "        current_progress = int(round(count/allproducts_length,2)*100)\n",
    "        current_runtime = round(time.time() - start_time,3)\n",
    "        print('[-] current progress:', current_progress, '%', 'and a runtime of', current_runtime, 'seconds.')\n",
    "\n",
    "    #setting a dict up with just one product, so we can inspect and ref it\n",
    "    aproduct = allproducts[aproduct]\n",
    "    #creating a dictionary entry for each product category\n",
    "    #also counting the occurances of each category\n",
    "    if 'categories' in aproduct:\n",
    "        for categories in aproduct['categories']:\n",
    "            for acategory in categories:\n",
    "                if acategory in allcategories:\n",
    "                    allcategories[acategory] += 1\n",
    "                if acategory not in allcategories:\n",
    "                    allcategories[acategory] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed for 2773 categories with a final runtime of 4.56 seconds.\n"
     ]
    }
   ],
   "source": [
    "#print a summary of the categories processed\n",
    "allcategories_length = len(allcategories)\n",
    "current_runtime = round(time.time() - start_time,3)\n",
    "print('Process completed for', allcategories_length, 'categories with a final runtime of', current_runtime, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 00 ] (3429257, 'Clothing, Shoes & Jewelry')\n",
      "[ 01 ] (1086181, 'Women')\n",
      "[ 02 ] (617092, 'Clothing')\n",
      "[ 03 ] (541681, 'Men')\n",
      "[ 04 ] (537761, 'Novelty, Costumes & More')\n",
      "[ 05 ] (432653, 'Shoes')\n",
      "[ 06 ] (339900, 'Novelty')\n",
      "[ 07 ] (268065, 'Shoes & Accessories: International Shipping Available')\n",
      "[ 08 ] (255454, 'Jewelry')\n",
      "[ 09 ] (174962, 'Accessories')\n",
      "[ 10 ] (97095, 'Girls')\n",
      "[ 11 ] (93596, 'Tops & Tees')\n",
      "[ 12 ] (87688, 'Dresses')\n",
      "[ 13 ] (84549, 'T-Shirts')\n",
      "[ 14 ] (82063, 'Boots')\n",
      "[ 15 ] (80302, 'Shirts')\n",
      "[ 16 ] (79897, 'Sandals')\n",
      "[ 17 ] (79545, 'Watches')\n",
      "[ 18 ] (77684, 'Boys')\n",
      "[ 19 ] (73507, 'Jewelry: International Shipping Available')\n"
     ]
    }
   ],
   "source": [
    "#create a sorted list of categories\n",
    "sortedlist = []\n",
    "#covert the dictionary to a list of tuples\n",
    "for acategory in allcategories:\n",
    "  sortedlist.append((allcategories[acategory],acategory))\n",
    "#sort the list\n",
    "sortedlist = sorted(sortedlist, reverse=True)\n",
    "#print the top x records in the list\n",
    "top_n = 20\n",
    "for item in range(0,top_n):\n",
    "  print('[',str(item).zfill(2),']', sortedlist[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8327 product records for Nike.\n"
     ]
    }
   ],
   "source": [
    "nike_categories = allcategories['Nike']\n",
    "print(nike_categories, 'product records for Nike.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract a List of Product Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] current progress: 7 % and a runtime of 0.21 seconds.\n",
      "[-] current progress: 13 % and a runtime of 0.443 seconds.\n",
      "[-] current progress: 20 % and a runtime of 0.657 seconds.\n",
      "[-] current progress: 27 % and a runtime of 0.874 seconds.\n",
      "[-] current progress: 33 % and a runtime of 1.083 seconds.\n",
      "[-] current progress: 40 % and a runtime of 1.291 seconds.\n",
      "[-] current progress: 47 % and a runtime of 1.498 seconds.\n",
      "[-] current progress: 53 % and a runtime of 1.7 seconds.\n",
      "[-] current progress: 60 % and a runtime of 1.904 seconds.\n",
      "[-] current progress: 67 % and a runtime of 2.104 seconds.\n",
      "[-] current progress: 73 % and a runtime of 2.307 seconds.\n",
      "[-] current progress: 80 % and a runtime of 2.504 seconds.\n",
      "[-] current progress: 86 % and a runtime of 2.697 seconds.\n",
      "[-] current progress: 93 % and a runtime of 2.889 seconds.\n",
      "[-] current progress: 100 % and a runtime of 3.069 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Now, go ahead and use the Variable Expolorer in Spyder to locate a brand\n",
    "#that has a lot of product entries! Alternatively, type allcategories['Brand name']\n",
    "#to get a count for a specific brand. For instance:\n",
    "#>>allcategories['Nike']\n",
    "#>> 8327\n",
    "#>>allcategories['adidas']\n",
    "#>> 8645\n",
    "\n",
    "#I'd reccommend at least 1.5k products, but you're welcome to try smaller counts\n",
    "#all I care about is whether you have at least 2k reviews when it's all said and done\n",
    "\n",
    "\n",
    "##Now we need to go through our newly first dictionary and extract out the\n",
    "##matching ASINs for Nike\n",
    "\n",
    "##First, create a set where we will store our ASINs\n",
    "##We choose a set here because we don't want duplicates\n",
    "allnikeasins = set()\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for areview in allproducts:\n",
    "    theproduct = allproducts[areview]\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        current_progress = int(round(count/allproducts_length,2)*100)\n",
    "        current_runtime = round(time.time() - start_time,3)\n",
    "        print('[-] current progress:', current_progress, '%', 'and a runtime of', current_runtime, 'seconds.')\n",
    "\n",
    "    #let's iterate fore each category for a product, again, any given product\n",
    "    #can be assigned multiple product categories,\n",
    "    for categories in theproduct['categories']:\n",
    "        #each category is actually encoded as a list (even though they should\n",
    "        #just be strings, so we need to iterate one more time)\n",
    "        for acategory in categories:\n",
    "            #checking to see if the product category matches Nike\n",
    "            #lowercasing the category string incase capitalization might get\n",
    "            #in the way of a match\n",
    "            if 'nike' in acategory.lower():\n",
    "                #let's go ahead and store it to our set of Nike ASINs\n",
    "                allnikeasins.add(theproduct['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed for 8327 records with a final runtime of 3.079 seconds.\n"
     ]
    }
   ],
   "source": [
    "#print a summary of the categories processed\n",
    "allnikeasins_length = len(allnikeasins)\n",
    "current_runtime = round(time.time() - start_time,3)\n",
    "print('Process completed for', allnikeasins_length, 'records with a final runtime of', current_runtime, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the ASINs out to a file as a checkpoint\n",
    "outputfile = open('%s/allasins.txt' % DATA_DIR, 'w')\n",
    "\n",
    "outputfile.write(','.join(allnikeasins))\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this assigns the filename we're trying to load in to a string variable\n",
    "review_file_path = '%s/reviews_Clothing_Shoes_and_Jewelry.json' % DATA_DIR\n",
    "loadedjson = open(review_file_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading review data to dictionary:\n",
      "[-] current progress: 500000 and a runtime of 33.901 seconds.\n",
      "[-] current progress: 1000000 and a runtime of 53.423 seconds.\n",
      "[-] current progress: 1500000 and a runtime of 73.157 seconds.\n",
      "[-] current progress: 2000000 and a runtime of 92.679 seconds.\n",
      "[-] current progress: 2500000 and a runtime of 120.684 seconds.\n",
      "[-] current progress: 3000000 and a runtime of 140.692 seconds.\n",
      "[-] current progress: 3500000 and a runtime of 160.421 seconds.\n",
      "[-] current progress: 4000000 and a runtime of 180.362 seconds.\n",
      "[-] current progress: 4500000 and a runtime of 230.885 seconds.\n",
      "[-] current progress: 5000000 and a runtime of 250.479 seconds.\n",
      "[-] current progress: 5500000 and a runtime of 270.205 seconds.\n",
      "completed load of review data to dictionary.\n"
     ]
    }
   ],
   "source": [
    "#2) - Parsing the review data\n",
    "#First, let's iterate through the data and store it as a python dictionary\n",
    "\n",
    "#let's set a counter to see how many products we have in the json\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "#loading the json file\n",
    "#we've always got to initiate dictionaries before we can use them\n",
    "allreviews = {}\n",
    "\n",
    "#each line of data here is a product and its metadata\n",
    "print('loading review data to dictionary:')\n",
    "for aline in loadedjson:\n",
    "    #creating a counter to know our progress in processing the entire catalog\n",
    "    count += 1\n",
    "    if count % 500000 == 0:\n",
    "        #we're only going to print our count every 100k, this way we don't spam\n",
    "        #our output console\n",
    "        current_runtime = round(time.time() - start_time,3)\n",
    "        print('[-] current progress:', count, 'and a runtime of', current_runtime, 'seconds.')\n",
    "    #interestingly enough, this data isn't true JSON, instead it's python\n",
    "    #dictionaries that have essentially been printed as text. It's odd, but if\n",
    "    #we read the documentaion, all we need to do to load a dictionary is use\n",
    "    #the eval function. https://www.programiz.com/python-programming/methods/built-in/eval\n",
    "    #eval takes whatever string is passed to it, and interprets it as python code\n",
    "    #and runs it. So here, it's exactly what we need to interpret a printed\n",
    "    #python dictionary\n",
    "\n",
    "    areview = eval(aline)\n",
    "\n",
    "    #making a dictionary entry with the iteration count as the review key\n",
    "    #and it's metadata as nested dictionaries\n",
    "    allreviews[count] = areview\n",
    "print('completed load of review data to dictionary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed for 5748920 of 5748920 records with a final runtime of 281.212 seconds.\n"
     ]
    }
   ],
   "source": [
    "#print a summary of the records processed\n",
    "allreviews_length = len(allreviews)\n",
    "current_runtime = round(time.time() - start_time,3)\n",
    "print('Process completed for', count, 'of', allreviews_length, 'records with a final runtime of', current_runtime, 'seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract a List of Reviews Related to the Product Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the list of Nike Asins\n",
    "\n",
    "allnikeasins = []\n",
    "allasins_file_path = '%s/allasins.txt' % DATA_DIR\n",
    "\n",
    "#open the file and load to a list\n",
    "for data in open(allasins_file_path, 'r'):\n",
    "  asins = data.split(',')\n",
    "  for anasin in asins:\n",
    "    allnikeasins.append(anasin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed for 8327 records.\n",
      "First 5 Asins in list: ['B007I7C3IK', 'B0098YXOES', 'B007MV6BNU', 'B0053HC6T8', 'B002Z34I94']\n"
     ]
    }
   ],
   "source": [
    "#print a summary of the records processed\n",
    "allnikeasins_length = len(allnikeasins)\n",
    "print('Process completed for', allnikeasins_length, 'records.')\n",
    "print('First 5 Asins in list:', allnikeasins[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading review data to dictionary:\n",
      "[-] current progress: 9 % and a runtime of 52.024 seconds.\n",
      "[-] current progress: 17 % and a runtime of 103.339 seconds.\n",
      "[-] current progress: 26 % and a runtime of 154.976 seconds.\n",
      "[-] current progress: 35 % and a runtime of 206.807 seconds.\n",
      "[-] current progress: 43 % and a runtime of 258.728 seconds.\n",
      "[-] current progress: 52 % and a runtime of 309.869 seconds.\n",
      "[-] current progress: 61 % and a runtime of 360.782 seconds.\n",
      "[-] current progress: 70 % and a runtime of 411.814 seconds.\n",
      "[-] current progress: 78 % and a runtime of 463.55 seconds.\n",
      "[-] current progress: 87 % and a runtime of 514.539 seconds.\n",
      "[-] current progress: 96 % and a runtime of 564.564 seconds.\n",
      "completed load of review data to dictionary.\n"
     ]
    }
   ],
   "source": [
    "#Now, we need to go through all the reviews and pick out the reviews that\n",
    "#correspond to the matching ASINs, that is reviews that are tied to Nike ASINs\n",
    "\n",
    "#let's set a counter to see how many products we have in the json\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "#loading the json file\n",
    "#we've always got to initiate dictionaries before we can use them\n",
    "nikereviews = {}\n",
    "\n",
    "#each line of data here is a product and its metadata\n",
    "print('loading review data to dictionary:')\n",
    "for areview in allreviews:\n",
    "  count += 1\n",
    "  if count % 500000 == 0:\n",
    "      current_progress = int(round(count/allreviews_length,2)*100)\n",
    "      current_runtime = round(time.time() - start_time,3)\n",
    "      print('[-] current progress:', current_progress, '%', 'and a runtime of', current_runtime, 'seconds.')\n",
    "  #setting current review as a dictionary, so we can easily reference its\n",
    "  #entries\n",
    "  thereview = allreviews[areview]\n",
    "  theasin = thereview['asin']\n",
    "  reviewerid = thereview['reviewerID']\n",
    "  if theasin in allnikeasins:\n",
    "      #im setting the key here as something unique. if we just did by asin\n",
    "      #we'd only have one review for each asin, with the last review the only\n",
    "      #one being stored\n",
    "      thekey = '%s.%s' % (theasin, reviewerid)\n",
    "      nikereviews[thekey] = thereview\n",
    "print('completed load of review data to dictionary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed for 5748920 of 21570 records with a final runtime of 589.318 seconds.\n"
     ]
    }
   ],
   "source": [
    "#print a summary of the records processed\n",
    "nikereviews_length = len(nikereviews)\n",
    "current_runtime = round(time.time() - start_time,3)\n",
    "print('Process completed for', count, 'of', nikereviews_length, 'records with a final runtime of', current_runtime, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our data to a JSON dictionary\n",
    "allnikereviews_file_path = '%s/allnikereviews.json' % DATA_DIR\n",
    "json.dump(nikereviews, open(allnikereviews_file_path, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview a Record from the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this assigns the filename we're trying to load\n",
    "allnikereviews_file_path = '%s/allnikereviews.json' % DATA_DIR\n",
    "json_file = json.load(open(allnikereviews_file_path, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reviewerID': 'A7H6Q3JAE3UTR', 'asin': 'B000G42Z2Q', 'reviewerName': 'april fritsche', 'helpful': [0, 0], 'reviewText': 'I bought these for my son and he loves them.  He said that that r very comfortable.  He said he wears them all the time.  He said that they look great.', 'overall': 5.0, 'summary': 'love them', 'unixReviewTime': 1388620800, 'reviewTime': '01 2, 2014'}\n",
      "{'reviewerID': 'A278R123DX0CLR', 'asin': 'B0013UXIAK', 'reviewerName': 'AQB', 'helpful': [0, 2], 'reviewText': 'Too much money for the pair. I saw a better pair for less price. I will shop around Zappos or Nike websites next time.', 'overall': 3.0, 'summary': 'its ok', 'unixReviewTime': 1377561600, 'reviewTime': '08 27, 2013'}\n",
      "{'reviewerID': 'A1CZIWT35UVJS6', 'asin': 'B001PA87L8', 'reviewerName': 'Courtney', 'helpful': [1, 1], 'reviewText': \"I love these shoes, they are perfect. Very comfy especially since I'm on my feet all day at work. It's wierd that they don't have a tongue but it only makes them fit better.\", 'overall': 5.0, 'summary': 'Comfy and cute', 'unixReviewTime': 1380067200, 'reviewTime': '09 25, 2013'}\n",
      "{'reviewerID': 'A24AX2P3PW6EBH', 'asin': 'B002FKHOOS', 'reviewerName': 'MattyBoy717', 'helpful': [9, 15], 'reviewText': \"I walked around in these for a month before I noticed that the right shoe was bigger than the left. Yeah, I kept feeling my foot slipping out as I walked, but I have a high arch, so I thought it was me. Finally, I gave them a good look side-by-side and I couldn't believe it. Even though the right shoe was imprinted with the correct shoe size, it was a full size bigger than the left one. And since it had been a month, it was too late to return them. I'm out $80. So, I'm letting myself look like a fool today to all of you out there so you won't make the same mistake I did. When you get your shoes, look at them carefully and make sure they match.\", 'overall': 2.0, 'summary': 'My Fault', 'unixReviewTime': 1371427200, 'reviewTime': '06 17, 2013'}\n",
      "{'reviewerID': 'A269BGMUL7YS1W', 'asin': 'B00310WE0U', 'reviewerName': 'Youn Sang Mo', 'helpful': [2, 4], 'reviewText': 'i am having great day after wearing this shoes. very satisfied and good. i would like to have another purchase.', 'overall': 5.0, 'summary': 'nice and quick', 'unixReviewTime': 1384732800, 'reviewTime': '11 18, 2013'}\n",
      "{'reviewerID': 'A39MEJK05JXJBZ', 'asin': 'B003PBU4Z2', 'reviewerName': 'souldivider', 'helpful': [0, 0], 'reviewText': 'I love this blazer models of Nike and I fell in love with this shoe when I first saw it. The shiny back, the greys, the pinks, they suit very well. I usually wear size 6.5 but only a size 7 was left in stock. Thinking that my other Nike was size 7, I decided to purchase this one. Result: Size 6.5 would be much better, however the puffy inside of this shoe made size 7 OK.', 'overall': 5.0, 'summary': 'Very cute and very cool', 'unixReviewTime': 1297036800, 'reviewTime': '02 7, 2011'}\n",
      "{'reviewerID': 'A32PRDGJLLJFV1', 'asin': 'B004HJT24K', 'reviewerName': 'tiani patterson', 'helpful': [0, 0], 'reviewText': 'I am very pleased, it is just what I expected.  My daughter had these shoes before but they were lost in a fire.  We saw these online and I decided to order them.  The product is exactley the same as if I bought them from the mall.  The delivery was on time, I am very pleased.', 'overall': 5.0, 'summary': 'Like I bought them from the store', 'unixReviewTime': 1355184000, 'reviewTime': '12 11, 2012'}\n",
      "{'reviewerID': 'A1RT1XBKWCT6PH', 'asin': 'B004TRMR1U', 'reviewerName': 'bpkc', 'helpful': [0, 3], 'reviewText': 'I purchased these cleats for my adult wooden bat baseball league. Although the cleats look nice....that is about it.Problems:Uncomfortable...I have worn them for at least 10 games total an I get blisters every time on the outside of my toes.The swoosh that you can switch the colors on....never stay in place....the move around and never match up with the the swoosh on the shoe.The tongue on my left shoe keeps sliding into my cleat...making it very uncomfortable....does not matter how tight I tie the shoe lace (yes i have the laces going through the loop in the middle of the tongue.)Just about every game i have a spike come loose and I have to re tighten them.This is just a bad cleat period. I would not recommend this to a regular ball player....there are far more effective and comfortable cleats out there.', 'overall': 2.0, 'summary': 'Too many problems', 'unixReviewTime': 1365465600, 'reviewTime': '04 9, 2013'}\n",
      "{'reviewerID': 'A1B7PCTWYWQSYV', 'asin': 'B00515GEJK', 'reviewerName': 'Marissa Gaytan', 'helpful': [0, 0], 'reviewText': \"This bag is a perfect fit.  It's not too big and not too small.\", 'overall': 5.0, 'summary': \"This bag is a perfect fit. It's not too big and not too ...\", 'unixReviewTime': 1405209600, 'reviewTime': '07 13, 2014'}\n",
      "{'reviewerID': 'AGP3D5ZB7N3NA', 'asin': 'B00594DYW8', 'reviewerName': 'S. M. Hughes \"mom of 2\"', 'helpful': [0, 0], 'reviewText': 'I absolutely love this pair of shoes. I am more prone to barefoot running, and the flexibility of these shoes are not at all restrictive on my feet and they feel great! I was so very happy to find them for a good price! Seller shipped very quickly and overall, the experience with this purchase was A+!', 'overall': 5.0, 'summary': 'Great Item, great seller!', 'unixReviewTime': 1322524800, 'reviewTime': '11 29, 2011'}\n"
     ]
    }
   ],
   "source": [
    "#select a random review\n",
    "count = 0\n",
    "for a_review in json_file:\n",
    "  count += 1\n",
    "  if count % 1000 == 0:\n",
    "    the_review = json_file[a_review]\n",
    "    print(the_review)\n",
    "    #sleep(10)\n",
    "  if count >= 10000:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reviewerID': 'AGP3D5ZB7N3NA',\n",
       " 'asin': 'B00594DYW8',\n",
       " 'reviewerName': 'S. M. Hughes \"mom of 2\"',\n",
       " 'helpful': [0, 0],\n",
       " 'reviewText': 'I absolutely love this pair of shoes. I am more prone to barefoot running, and the flexibility of these shoes are not at all restrictive on my feet and they feel great! I was so very happy to find them for a good price! Seller shipped very quickly and overall, the experience with this purchase was A+!',\n",
       " 'overall': 5.0,\n",
       " 'summary': 'Great Item, great seller!',\n",
       " 'unixReviewTime': 1322524800,\n",
       " 'reviewTime': '11 29, 2011'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the review to the screen\n",
    "the_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract a List of Products Related to Product Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the list of Nike Asins\n",
    "\n",
    "allnikeasins = []\n",
    "allasins_file_path = '%s/allasins.txt' % DATA_DIR\n",
    "\n",
    "#open the file and load to a list\n",
    "for data in open(allasins_file_path, 'r'):\n",
    "  asins = data.split(',')\n",
    "  for anasin in asins:\n",
    "    allnikeasins.append(anasin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed for 8327 records.\n",
      "First 5 Asins in list: ['B007I7C3IK', 'B0098YXOES', 'B007MV6BNU', 'B0053HC6T8', 'B002Z34I94']\n"
     ]
    }
   ],
   "source": [
    "#print a summary of the records processed\n",
    "allnikeasins_length = len(allnikeasins)\n",
    "print('Process completed for', allnikeasins_length, 'records.')\n",
    "print('First 5 Asins in list:', allnikeasins[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the path for the all product dict\n",
    "allproducts_file_path = '%s/allproducts.p' % DATA_DIR\n",
    "#load the dict\n",
    "allproducts =  pickle.load(open(allproducts_file_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the full product catelog: 1503384\n",
      "size of the nike product catelog: 8327\n"
     ]
    }
   ],
   "source": [
    "print('size of the full product catelog:', len(allproducts))\n",
    "keys = set(allnikeasins).intersection(allproducts)\n",
    "allnikeproducts = {key:allproducts[key] for key in keys}\n",
    "print('size of the nike product catelog:', len(allnikeproducts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the files to disk\n",
    "allnikeproducts_file_path = '%s/allnikeproducts.p' % DATA_DIR\n",
    "pickle.dump(allnikeproducts, open(allnikeproducts_file_path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data\n",
    "### Load the Nike Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this assigns the filename we're trying to load\n",
    "allnikereviews_file_path = '%s/allnikereviews.json' % DATA_DIR\n",
    "json_file = json.load(open(allnikereviews_file_path, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract review text from all review details\n",
    "reviews = []\n",
    "for a_review in json_file:\n",
    "    the_review = json_file[a_review]\n",
    "    text = the_review[\"reviewText\"]\n",
    "    reviews.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "product-review",
   "language": "python",
   "name": "product-review"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
