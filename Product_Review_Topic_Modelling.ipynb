{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaySZigQAc_l"
   },
   "source": [
    "# Product Review Topic Modelling\n",
    "\n",
    "A report focused on Topic Modeling using [tmtoolkit](https://tmtoolkit.readthedocs.io/) to process text review data from Amazon for clothing and shoes manufactured by Nike. Use unsupervised LDA clustering methods to create popular topics in the Amazon review data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEuxfGTAAEWQ"
   },
   "source": [
    "**Context**\n",
    "Online marketplaces and applications can product data at volumes too big for a human to read and analyze. An example of this is Amazon, where millions of customers buy and review products. When a company wants to extract insights from their reviews, they need a way to process the text of the reviews and identifty patterns. This exploratory analysis of big data is a common use case for Topic Modeling using unsupervised learning.\n",
    "\n",
    "**Background**\n",
    "The Nike brand is interested in extracting key indsights from customer review data on the Amazon website. Specifically, they would like to understand when customers are not satisfied and what major problems they face with their products. To accomplish this task, the Data Science team at Nike is tasked with analyzing the Amazon review data. First, the team must identify Nike's product ASINs on Amazon and extract the relevant reviews. Next, the team will complete topic modeling on the text of the reviews using the LDA clustering method to model the data into popular topics. Finally, the team will vizualize the topics and derive initial insights to drive the strategy of the team.\n",
    "\n",
    "** LDA (Latent Dirichleyt Allocation)**\n",
    "This project will focus on the application of [lda.LDA](https://github.com/lda-project/lda), which implements latent Dirichlet allocation (LDA).\n",
    "\n",
    "Specifically, in natural language processing (NLP), Latent Dirichlet Allocation (LDA) is a Bayesian network used for topic modeling. It aims to explain a set of observations through unobserved groups. Each group explains why some parts of the data are similar. For most purposes, observations are words and are collected into documents. Each word's presence is attributed to one of the document's topics. Each document will contain a small number of topics. [1]\n",
    "\n",
    "**Data Source**\n",
    "Prof. Julian McAuley at UC-San Diego created an [Amazon Product Data](http://jmcauley.ucsd.edu/data/amazon/links.html) database, which contains Amazon products details. This project will leverage 2 datasets from the database:\n",
    "- (1) meta-data about products - details related to each Amazon product\n",
    "- (2) product reviews - reviews on all types of Amazon products\n",
    "\n",
    "Since the overall size of these datasets are huge (~80gb), a subset of data will be utilized to focus on the product category of **Clothing, Shoes & Jewelry**.\n",
    "\n",
    "The raw data sources for the project can be accessed with the following links:\n",
    "- [Product Data](http://128.138.93.164/meta_Clothing_Shoes_and_Jewelry.json.gz)\n",
    "- [Review Data](http://128.138.93.164/reviews_Clothing_Shoes_and_Jewelry.json.gz)\n",
    "\n",
    "**Overview of Observations**\n",
    "\n",
    "The **Product Details** dataset has 1,503,384 records in a json format and includes the following fields:\n",
    "- asin: a unique product id assigned to the Amazon product\n",
    "- title: the name of the product displayed on the amazon page\n",
    "- imUrl: a URL to access the product page\n",
    "- related: related products to the this product\n",
    "- salesRank: the main sales category and the product rank within the sales category\n",
    "- categories: categories or tags grouping the products\n",
    "\n",
    "The **Review Details** dataset has 5,748,920 records in a json format and includes the following fields:\n",
    "- reviewerID': a unique id assigned to the review\n",
    "- asin': a unique id of the product reviewed\n",
    "- reviewerName': the text name of the user who created the review\n",
    "- helpful': indicator if the review was helpful or not helpful\n",
    "- reviewText': the text written for the review\n",
    "- overall': the rating of the product (in stars) on a scale of 1-5\n",
    "- summary': the summary or title of the review\n",
    "- unixReviewTime: the numeric unix representation of the time the review was created\n",
    "- reviewTime: the time the review was created as a formatted data string\n",
    "\n",
    "**Objective**\n",
    "The objective is to build a unsupervize topic model for nike product reviews. The topic model will group reviews into topics and create label to identify patterns and trends in user reviews. Final evaluation will use qualitative and look at value of the topics modeled and the usefullness of insights extracted from the topics.\n",
    "\n",
    "**Report Overview**\n",
    "The project will cover 5 key phases:\n",
    "1. Data Source: Extracting, filtering, and focusing the data on the Nike brand\n",
    "2. Preprocessing: Extracting Word Features with Natural Language Processing (NLP) tools\n",
    "3. Parameter Tuning: Tuning the topic model parameters to improve the purity and uniqueness of topics\n",
    "4. Final Model: Building the final topic model\n",
    "5. Classify and Enright Topic Data: tuning the topic labels, tagging all records, and adding in supporting data to the final dataset\n",
    "6. Model Evaluation: Review the model and extract insights for the Nike Brand\n",
    "7. Results: Review the findings of the Topic Evaluation.\n",
    "\n",
    "## Setting Up\n",
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4DfKV1oAY5o",
    "outputId": "4ffe07ce-a6f3-4a53-ca61-cd97f7e184f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] Importing packages...\n"
     ]
    }
   ],
   "source": [
    "# Special Install of Packages\n",
    "print('[-] Importing packages...')\n",
    "#special_install_tmtoolkit\n",
    "import os\n",
    "try:\n",
    "    import tmtoolkit\n",
    "except:\n",
    "  print('starting patch of tmtoolkit.')\n",
    "  !pip install --quiet -U \"tmtoolkit[recommended,lda,sklearn,wordclouds,topic_modeling_eval_extra]\"\n",
    "  print('finished patch of tmtoolkit.')\n",
    "  os.kill(os.getpid(), 9)\n",
    "\n",
    "#special_install_lda\n",
    "import os\n",
    "try:\n",
    "  from tmtoolkit.topicmod.tm_lda import compute_models_parallel\n",
    "except:\n",
    "  !pip install --quiet tmtoolkit['lda']\n",
    "  from tmtoolkit.topicmod.tm_lda import compute_models_parallel\n",
    "\n",
    "try:\n",
    "  from lda import LDA\n",
    "except:\n",
    "  !pip install --quiet lda\n",
    "  from lda import LDA\n",
    "\n",
    "#special_install_pyLDAvis\n",
    "try:\n",
    "  import pyLDAvis\n",
    "except:\n",
    "  !pip install --quiet pyLDAvis==2.1.2\n",
    "  import pyLDAvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VoJRWDFiAqqe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] Importing packages...\n"
     ]
    }
   ],
   "source": [
    "print('[-] Importing packages...')\n",
    "# File Connection and File Manipulation\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "# Import Usability Functions\n",
    "import logging\n",
    "import warnings\n",
    "# Basic Data Science Toolkits\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from time import sleep\n",
    "# Basic Data Vizualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Text Preprocessing (tmtoolkit)\n",
    "import tmtoolkit\n",
    "from tmtoolkit.corpus import Corpus, lemmatize, to_lowercase, remove_chars, filter_clean_tokens\n",
    "from tmtoolkit.corpus import filter_for_pos, remove_common_tokens, remove_uncommon_tokens\n",
    "from tmtoolkit.corpus import corpus_num_tokens, corpus_tokens_flattened\n",
    "from tmtoolkit.corpus import doc_tokens, tokens_table, doc_labels, dtm\n",
    "from tmtoolkit.corpus import vocabulary, vocabulary_size, vocabulary_counts\n",
    "from tmtoolkit.topicmod.model_io import print_ldamodel_topic_words\n",
    "from tmtoolkit.topicmod.tm_lda import compute_models_parallel\n",
    "from tmtoolkit.corpus.visualize import plot_doc_lengths_hist, plot_doc_frequencies_hist, plot_ranked_vocab_counts\n",
    "#https://tmtoolkit.readthedocs.io/en/latest/preprocessing.html\n",
    "# Text Preprocessing(other)\n",
    "from string import punctuation\n",
    "import nltk\n",
    "import scipy.sparse\n",
    "# Topic Modeling\n",
    "from lda import LDA\n",
    "import pyLDAvis\n",
    "from tmtoolkit.topicmod import tm_lda\n",
    "from tmtoolkit.topicmod.tm_lda import compute_models_parallel\n",
    "from tmtoolkit.topicmod.model_io import print_ldamodel_topic_words\n",
    "from tmtoolkit.topicmod.model_io import save_ldamodel_to_pickle\n",
    "from tmtoolkit.topicmod.model_io import load_ldamodel_from_pickle\n",
    "from tmtoolkit.topicmod.model_io import ldamodel_top_doc_topics\n",
    "from tmtoolkit.topicmod.evaluate import results_by_parameter\n",
    "from tmtoolkit.topicmod.visualize import plot_eval_results\n",
    "from tmtoolkit.topicmod.visualize import parameters_for_ldavis\n",
    "from tmtoolkit.topicmod.visualize import generate_wordclouds_for_topic_words\n",
    "from tmtoolkit.topicmod.model_stats import generate_topic_labels_from_top_words\n",
    "from tmtoolkit.bow.bow_stats import doc_lengths\n",
    "# Sentiment Modeling\n",
    "from textblob import TextBlob\n",
    "# normalize\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGSpO4krARX9"
   },
   "source": [
    "### Set Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dWA_xDHjAyA_"
   },
   "outputs": [],
   "source": [
    "random.seed(20191120)   # to make the sampling reproducible\n",
    "np.set_printoptions(precision=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIK88FsNAeT7"
   },
   "source": [
    "### Verify GPU Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FJ6fPCw7AZcz",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Mar 25 16:34:07 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.52                 Driver Version: 551.52         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   47C    P8             13W /  115W |     599MiB /   8192MiB |     12%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2024    C+G   C:\\Windows\\System32\\dwm.exe                 N/A      |\n",
      "|    0   N/A  N/A      8152    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     16440    C+G   ...x86__97hta09mmv6hy\\Build\\Lively.exe      N/A      |\n",
      "|    0   N/A  N/A     16764    C+G   ...Brave-Browser\\Application\\brave.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HONfWtNGCAlQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is created!\n",
      "The evaluation directory is created!\n",
      "The model directory is created!\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = \"./\"\n",
    "DATA_DIR = \"%s/data\" % ROOT_DIR\n",
    "EVAL_DIR = \"%s/evaluation\" % ROOT_DIR\n",
    "MODEL_DIR = \"%s/models\" % ROOT_DIR\n",
    "\n",
    "#Create missing directories, if they don't exist\n",
    "if not os.path.exists(DATA_DIR):\n",
    "  # Create a new directory because it does not exist\n",
    "  os.makedirs(DATA_DIR)\n",
    "  print(\"The data directory is created!\")\n",
    "if not os.path.exists(EVAL_DIR):\n",
    "  # Create a new directory because it does not exist\n",
    "  os.makedirs(EVAL_DIR)\n",
    "  print(\"The evaluation directory is created!\")\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "  # Create a new directory because it does not exist\n",
    "  os.makedirs(MODEL_DIR)\n",
    "  print(\"The model directory is created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mG2bHve1_7Ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/meta_Clothing_Shoes_and_Jewelry.jsonl.gz',\n",
       " <http.client.HTTPMessage at 0x1e28ab25790>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#link would be expired by the time of the grading. It's just here to show how it could be done.\n",
    "import urllib.request\n",
    "url = 'https://d3c33hcgiwev3.cloudfront.net/F4BxPB4wTWSAcTweMH1ktA_cf7170556c97498f80af8f7b869f35f1_meta_Clothing_Shoes_and_Jewelry.jsonl.gz?Expires=1711497600&Signature=ABbwkDeNeTL-s2uHXlyu4QweI8JnGGaCopj50~hba0MaEMhk70IImmREl5hzHC6O0mxPkFACpq5PP~JX8XnIEVe1cph7G2neQ0zgW1Q9npt47H-RMjk0p6emo95c8oZ~aWty9kEQBZM3D7PqnwRe7dHRWbZ-eS84gYkvdAq0w48_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A'\n",
    "filename = './data/meta_Clothing_Shoes_and_Jewelry.jsonl.gz'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/reviews_Clothing_Shoes_and_Jewelry.json.gz',\n",
       " <http.client.HTTPMessage at 0x1e28b6f9ed0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://d3c33hcgiwev3.cloudfront.net/ed_p_0DhRh-f6f9A4dYfJA_d374df8f88084b8c9384c9b910b50cf1_reviews_Clothing_Shoes_and_Jewelry.json.gz?Expires=1711497600&Signature=Nllx8wiA27Ey2uOdCpLCRGy-y31pBWRnlXG5AtnpXxVoxPLABKJBTsk4eAcUIGTPzw23f6zckeNTtAbIfOy9w8mCGa4kjFDZAKFAaLXzcmUEs4ia09dzaSYjuduIHhE4ECVvr~VyutJkqfS4MwXxWpgqvwbB44F9RqE5g3P3h6A_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A'\n",
    "filename = './data/reviews_Clothing_Shoes_and_Jewelry.json.gz'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fiiUVkmJAOch"
   },
   "outputs": [],
   "source": [
    "meta_file_path = '%s/meta_Clothing_Shoes_and_Jewelry.jsonl.gz' % DATA_DIR\n",
    "review_file_path = '%s/reviews_Clothing_Shoes_and_Jewelry.json.gz' % DATA_DIR\n",
    "\n",
    "!gzip -d \"$meta_file_path\"\n",
    "!gzip -d \"$review_file_path\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Product Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##this assigns the filename we're trying to load in to a string variable\n",
    "meta_file_path = '%s/meta_Clothing_Shoes_and_Jewelry.jsonl' % DATA_DIR\n",
    "loadedjson = open(meta_file_path, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading product data to dictionary:\n",
      "[-] current progress: 100000 and a runtime of 10.546 seconds.\n",
      "[-] current progress: 200000 and a runtime of 21.292 seconds.\n",
      "[-] current progress: 300000 and a runtime of 32.16 seconds.\n",
      "[-] current progress: 400000 and a runtime of 43.266 seconds.\n",
      "[-] current progress: 500000 and a runtime of 54.889 seconds.\n",
      "[-] current progress: 600000 and a runtime of 67.461 seconds.\n",
      "[-] current progress: 700000 and a runtime of 80.688 seconds.\n",
      "[-] current progress: 800000 and a runtime of 92.264 seconds.\n",
      "[-] current progress: 900000 and a runtime of 107.114 seconds.\n",
      "[-] current progress: 1000000 and a runtime of 118.509 seconds.\n",
      "[-] current progress: 1100000 and a runtime of 134.945 seconds.\n",
      "[-] current progress: 1200000 and a runtime of 148.039 seconds.\n",
      "[-] current progress: 1300000 and a runtime of 165.82 seconds.\n",
      "[-] current progress: 1400000 and a runtime of 179.821 seconds.\n",
      "[-] current progress: 1500000 and a runtime of 193.808 seconds.\n"
     ]
    }
   ],
   "source": [
    "#The data used in this script comes from: http://jmcauley.ucsd.edu/data/amazon/links.html\n",
    "#The data here is the 'per category' data for Clothing, Shoes and Jewelry\n",
    "#use the above url to better understand the data, where it came from, and some\n",
    "#tips on how to use it!\n",
    "\n",
    "#getting reviews is going to be a two step process:\n",
    "#1) go through the amazon product catalog for \"Clothing, Shoes and Jewelery\n",
    "#and extract out matching products by their ASIN\n",
    "#2) go through the review data and parse out the matching reviews by ASIN\n",
    "\n",
    "#1) - Extracting ASINs by brand\n",
    "#First, let's iterate through the data and store it as a python dictionary\n",
    "\n",
    "#let's set a counter to see how many products we have in the json\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "#loading the json file\n",
    "#we've always got to initiate dictionaries before we can use them\n",
    "allproducts = {}\n",
    "\n",
    "#each line of data here is a product and its metadata\n",
    "print('loading product data to dictionary:')\n",
    "for aline in loadedjson:\n",
    "    #creating a counter to know our progress in processing the entire catalog\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        #we're only going to print our count every 100k, this way we don't spam\n",
    "        #our output console\n",
    "        current_runtime = round(time.time() - start_time,3)\n",
    "        print('[-] current progress:', count, 'and a runtime of', current_runtime, 'seconds.')\n",
    "    #interestingly enough, this data isn't true JSON, instead it's python\n",
    "    #dictionaries that have essentially been printed as text. It's odd, but if\n",
    "    #we read the documentaion, all we need to do to load a dictionary is use\n",
    "    #the eval function. https://www.programiz.com/python-programming/methods/built-in/eval\n",
    "    #eval takes whatever string is passed to it, and interprets it as python code\n",
    "    #and runs it. So here, it's exactly what we need to interpret a printed\n",
    "    #python dictionary\n",
    "\n",
    "    aproduct = eval(aline)\n",
    "\n",
    "    #making a dictionary entry with the ASIN of the product as the key\n",
    "    #and it's metadata as nested dictionaries\n",
    "    allproducts[aproduct['asin']] = aproduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed for 1503384 of 1503384 records with a final runtime of 686.552 seconds.\n"
     ]
    }
   ],
   "source": [
    "#print a summary of the records processed\n",
    "allproducts_length = len(allproducts)\n",
    "current_runtime = round(time.time() - start_time,3)\n",
    "print('Process completed for', count, 'of', allproducts_length, 'records with a final runtime of', current_runtime, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'asin': 'B00KUSKHDC',\n",
       " 'title': \"Family Guy - Men's T-shirt Evil Monkey\",\n",
       " 'imUrl': 'http://ecx.images-amazon.com/images/I/41eUK6CAY4L._SX342_.jpg',\n",
       " 'related': {'also_viewed': ['B004P0JEK8',\n",
       "   'B00EC4UZ3M',\n",
       "   'B000VYZEY2',\n",
       "   'B00HZSI7QE']},\n",
       " 'salesRank': {'Clothing': 288020},\n",
       " 'categories': [['Clothing, Shoes & Jewelry', 'Men'],\n",
       "  ['Clothing, Shoes & Jewelry',\n",
       "   'Novelty, Costumes & More',\n",
       "   'Novelty',\n",
       "   'Clothing',\n",
       "   'Men',\n",
       "   'Shirts',\n",
       "   'T-Shirts']]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preview the product record\n",
    "allproducts['B00KUSKHDC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the files to disk\n",
    "allproducts_file_path = '%s/allproducts.p' % DATA_DIR\n",
    "pickle.dump(allproducts, open(allproducts_file_path, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the Product Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading categories data to dictionary:\n",
      "[-] current progress: 7 % and a runtime of 0.323 seconds.\n",
      "[-] current progress: 13 % and a runtime of 0.666 seconds.\n",
      "[-] current progress: 20 % and a runtime of 0.972 seconds.\n",
      "[-] current progress: 27 % and a runtime of 1.254 seconds.\n",
      "[-] current progress: 33 % and a runtime of 1.542 seconds.\n",
      "[-] current progress: 40 % and a runtime of 1.824 seconds.\n",
      "[-] current progress: 47 % and a runtime of 2.101 seconds.\n",
      "[-] current progress: 53 % and a runtime of 2.381 seconds.\n",
      "[-] current progress: 60 % and a runtime of 2.656 seconds.\n",
      "[-] current progress: 67 % and a runtime of 2.92 seconds.\n",
      "[-] current progress: 73 % and a runtime of 3.188 seconds.\n",
      "[-] current progress: 80 % and a runtime of 3.457 seconds.\n",
      "[-] current progress: 86 % and a runtime of 3.717 seconds.\n",
      "[-] current progress: 93 % and a runtime of 3.974 seconds.\n",
      "[-] current progress: 100 % and a runtime of 4.222 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Next we need to explore the product data to see what categories are common in the\n",
    "#data. As you'll learn, product categories are wishywashy in that they can be\n",
    "#product categories (e.g., baby, house and home), or they can be brands!\n",
    "#We're already dealing with a subset of the product categories, Clothing, Shoes\n",
    "#and Jewlery. We still need to find a list of product ids for our specific\n",
    "#brand. To do this,We're going to use the 'categories' metadata field to find\n",
    "#your brand\n",
    "\n",
    "##Let's create a dictionary of all the product subcategories\n",
    "#and by doing so, also come up with a list of brands and the number of products\n",
    "#they have listed in the amazon product catalog\n",
    "\n",
    "allcategories = {}\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "#each line of data here is a product and its metadata\n",
    "print('loading categories data to dictionary:')\n",
    "for aproduct in allproducts:\n",
    "    #creating a counter to know our progress in processing the entire catalog\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        #we now know there are 1.5 million products, so we can build a counter\n",
    "        #that tells how our processing is going. When the counter reaches one\n",
    "        #we're done!\n",
    "        current_progress = int(round(count/allproducts_length,2)*100)\n",
    "        current_runtime = round(time.time() - start_time,3)\n",
    "        print('[-] current progress:', current_progress, '%', 'and a runtime of', current_runtime, 'seconds.')\n",
    "\n",
    "    #setting a dict up with just one product, so we can inspect and ref it\n",
    "    aproduct = allproducts[aproduct]\n",
    "    #creating a dictionary entry for each product category\n",
    "    #also counting the occurances of each category\n",
    "    if 'categories' in aproduct:\n",
    "        for categories in aproduct['categories']:\n",
    "            for acategory in categories:\n",
    "                if acategory in allcategories:\n",
    "                    allcategories[acategory] += 1\n",
    "                if acategory not in allcategories:\n",
    "                    allcategories[acategory] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed for 2773 categories with a final runtime of 15.391 seconds.\n"
     ]
    }
   ],
   "source": [
    "#print a summary of the categories processed\n",
    "allcategories_length = len(allcategories)\n",
    "current_runtime = round(time.time() - start_time,3)\n",
    "print('Process completed for', allcategories_length, 'categories with a final runtime of', current_runtime, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 00 ] (3429257, 'Clothing, Shoes & Jewelry')\n",
      "[ 01 ] (1086181, 'Women')\n",
      "[ 02 ] (617092, 'Clothing')\n",
      "[ 03 ] (541681, 'Men')\n",
      "[ 04 ] (537761, 'Novelty, Costumes & More')\n",
      "[ 05 ] (432653, 'Shoes')\n",
      "[ 06 ] (339900, 'Novelty')\n",
      "[ 07 ] (268065, 'Shoes & Accessories: International Shipping Available')\n",
      "[ 08 ] (255454, 'Jewelry')\n",
      "[ 09 ] (174962, 'Accessories')\n",
      "[ 10 ] (97095, 'Girls')\n",
      "[ 11 ] (93596, 'Tops & Tees')\n",
      "[ 12 ] (87688, 'Dresses')\n",
      "[ 13 ] (84549, 'T-Shirts')\n",
      "[ 14 ] (82063, 'Boots')\n",
      "[ 15 ] (80302, 'Shirts')\n",
      "[ 16 ] (79897, 'Sandals')\n",
      "[ 17 ] (79545, 'Watches')\n",
      "[ 18 ] (77684, 'Boys')\n",
      "[ 19 ] (73507, 'Jewelry: International Shipping Available')\n"
     ]
    }
   ],
   "source": [
    "#create a sorted list of categories\n",
    "sortedlist = []\n",
    "#covert the dictionary to a list of tuples\n",
    "for acategory in allcategories:\n",
    "  sortedlist.append((allcategories[acategory],acategory))\n",
    "#sort the list\n",
    "sortedlist = sorted(sortedlist, reverse=True)\n",
    "#print the top x records in the list\n",
    "top_n = 20\n",
    "for item in range(0,top_n):\n",
    "  print('[',str(item).zfill(2),']', sortedlist[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8327 product records for Nike.\n"
     ]
    }
   ],
   "source": [
    "nike_categories = allcategories['Nike']\n",
    "print(nike_categories, 'product records for Nike.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] current progress: 7 % and a runtime of 0.204 seconds.\n",
      "[-] current progress: 13 % and a runtime of 0.44 seconds.\n",
      "[-] current progress: 20 % and a runtime of 0.655 seconds.\n",
      "[-] current progress: 27 % and a runtime of 0.861 seconds.\n",
      "[-] current progress: 33 % and a runtime of 1.066 seconds.\n",
      "[-] current progress: 40 % and a runtime of 1.275 seconds.\n",
      "[-] current progress: 47 % and a runtime of 1.461 seconds.\n",
      "[-] current progress: 53 % and a runtime of 1.644 seconds.\n",
      "[-] current progress: 60 % and a runtime of 1.829 seconds.\n",
      "[-] current progress: 67 % and a runtime of 2.024 seconds.\n",
      "[-] current progress: 73 % and a runtime of 2.23 seconds.\n",
      "[-] current progress: 80 % and a runtime of 2.412 seconds.\n",
      "[-] current progress: 86 % and a runtime of 2.613 seconds.\n",
      "[-] current progress: 93 % and a runtime of 2.803 seconds.\n",
      "[-] current progress: 100 % and a runtime of 2.982 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Now, go ahead and use the Variable Expolorer in Spyder to locate a brand\n",
    "#that has a lot of product entries! Alternatively, type allcategories['Brand name']\n",
    "#to get a count for a specific brand. For instance:\n",
    "#>>allcategories['Nike']\n",
    "#>> 8327\n",
    "#>>allcategories['adidas']\n",
    "#>> 8645\n",
    "\n",
    "#I'd reccommend at least 1.5k products, but you're welcome to try smaller counts\n",
    "#all I care about is whether you have at least 2k reviews when it's all said and done\n",
    "\n",
    "\n",
    "##Now we need to go through our newly first dictionary and extract out the\n",
    "##matching ASINs for Nike\n",
    "\n",
    "##First, create a set where we will store our ASINs\n",
    "##We choose a set here because we don't want duplicates\n",
    "allnikeasins = set()\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for areview in allproducts:\n",
    "    theproduct = allproducts[areview]\n",
    "    count += 1\n",
    "    if count % 100000 == 0:\n",
    "        current_progress = int(round(count/allproducts_length,2)*100)\n",
    "        current_runtime = round(time.time() - start_time,3)\n",
    "        print('[-] current progress:', current_progress, '%', 'and a runtime of', current_runtime, 'seconds.')\n",
    "\n",
    "    #let's iterate fore each category for a product, again, any given product\n",
    "    #can be assigned multiple product categories,\n",
    "    for categories in theproduct['categories']:\n",
    "        #each category is actually encoded as a list (even though they should\n",
    "        #just be strings, so we need to iterate one more time)\n",
    "        for acategory in categories:\n",
    "            #checking to see if the product category matches Nike\n",
    "            #lowercasing the category string incase capitalization might get\n",
    "            #in the way of a match\n",
    "            if 'nike' in acategory.lower():\n",
    "                #let's go ahead and store it to our set of Nike ASINs\n",
    "                allnikeasins.add(theproduct['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process completed for 8327 records with a final runtime of 10.367 seconds.\n"
     ]
    }
   ],
   "source": [
    "#print a summary of the categories processed\n",
    "allnikeasins_length = len(allnikeasins)\n",
    "current_runtime = round(time.time() - start_time,3)\n",
    "print('Process completed for', allnikeasins_length, 'records with a final runtime of', current_runtime, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the ASINs out to a file as a checkpoint\n",
    "outputfile = open('%s/allasins.txt' % DATA_DIR, 'w')\n",
    "\n",
    "outputfile.write(','.join(allnikeasins))\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "product-review",
   "language": "python",
   "name": "product-review"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
